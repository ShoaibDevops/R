---
title: "R Notebook"
output: html_notebook

---


__Bike Data Prepration__
```{r}
data <- read.csv("hour.csv")[,-1]
head(data)
str(data)
```


```{r}
data$yr <- as.factor(ifelse(data$yr == 0, '2011', '2012'))
data$mnth <- as.factor(months(as.Date(data$dteday), 
                              abbreviate = TRUE))
data$hr <- factor(data$hr)
data$weekday <- as.factor(weekdays(as.Date(data$dteday)))
data$season <- as.factor(ifelse(data$season == 1, 'Spring',
                                ifelse(data$season == 2, 'Summer',
                                       ifelse(data$season == 3, 'Fall', 'Winter'))))
data$weathersit <- as.factor(ifelse(data$weathersit == 1, 'Good',
                                    ifelse(data$weathersit == 2, 'Fair',
                                           ifelse(data$weathersit == 3, 'Bad', 'Very Bad'))))
data$holiday<-as.factor(ifelse(data$holiday == 0, 'No', 'Yes'))
data$workingday<-as.factor(ifelse(data$workingday == 0, 'No','Yes'))

```


This change is not required, but it will allow me to differentiate between the new and the total number of registrations.

```{r}
#changing the col name 
names(data)[names(data) == "registered"] <- "new"
names(data)[names(data) == "cnt"] <- "total"
head(data)
```


Finally, for the last step of the data preparation, I denormalized the values of the variables “temp”, “atemp”, “hum”, and “windspeed”; so that later I can analyze the real observations on the exploratory data analysis and in the prediction model.

```{r}
#Denormalizing the values
    #Temperature
for (i in 1:nrow(data)){
  tn = data[i, 10]
  t = (tn * (39 - (-8))) + (-8)
  data[i, 10] <- t
}
    #Feeling temperature
for (i in 1:nrow(data)){
  tn = data[i, 11]
  t = (tn * (50 - (-16))) + (-16)
  data[i, 11] <- t
}
    #Humidity
data$hum <- data$hum * 100
    #Wind speed
data$windspeed <- data$windspeed * 67

str(data)
```
Furthermore, I will use the new file to build the graphs of the R Shiny application.
```{r}
#Write the new file
data <- data[-1]
write.csv(data, "bike_sharing.csv", row.names = FALSE)
head(data)
```

__Modeling__
The columns “season” and “workingday” are dropped because the variables “mnth” and “weekday” can provide the same information. By indicating the month, we know which season of the year corresponds. Also, by providing information about the day of the week, we identify if it is a working day or not.


```{r}
 #Dropping columns
data <- data[c(-1,-2,-7,-13,-14)]
#head(data)
```

__Splitting the data__
Now that the data is ready for modeling, I proceed to split the data into train and test set. Additionally, to use the data for the R Shiny application, I saved these datasets into a new file. I used a split ratio of 0.8, meaning that the train data will contain 80% of the total observations and the test the remaining 20%.

```{r}
library(caTools)
set.seed(123)
split = sample.split(data$total, SplitRatio = 0.8)
train_set = subset(data, split == TRUE)
test_set = subset(data, split == FALSE)

#Write new files for the train and test sets

write.csv(train_set, "bike_train.csv", row.names = FALSE)
write.csv(test_set, "bike_test.csv", row.names = FALSE)


```



__Models__
Moreover, I continued to select the model for the prediction. Since the dependent variable is numeric (total registrations), it means that we are in the presence of a regression task. For this reason, I pre-selected multilinear regression, decision tree, and random forest models to predict the outcome of the dependent variable.
For the next step, I created the regression model with the training data. Then, I evaluated its performance by calculating the mean absolute error (MAE) and root mean square error (RMSE).

__
_#install.packages("Metrics")
library(Metrics)

  #Multilinear regression
multi = lm(formula = total ~., data = train_set)
    #Predicting the test values
y_pred_m = predict(multi, newdata = test_set)
    #Performance metrics
mae_m = mae(test_set[[10]], y_pred_m)
rmse_m = rmse(test_set[[10]], y_pred_m)
mae_m
rmse_m
__

```{r}
#Decision tree
library(rpart)
dt = rpart(formula = total ~ ., data = train_set,
           control = rpart.control(minsplit = 3))
    #Predicting the test values
y_pred_dt = predict(dt, newdata = test_set)
    #Performance metrics
mae_dt = mae(test_set[[10]], y_pred_dt)
rmse_dt = rmse(test_set[[10]], y_pred_dt)
mae_dt
rmse_dt
```



```{r}
#Random forest
library(randomForest)
set.seed(123)
rf = randomForest(formula = total ~ ., data = train_set,
                  ntree = 100)
    #Predicting the test values
y_pred_rf = predict(rf, newdata = test_set)
    #Performance metrics
mae_rf = mae(test_set[[10]], y_pred_rf)
rmse_rf = rmse(test_set[[10]], y_pred_rf)
mae_rf
rmse_rf
```

Once I have the performance metrics of all the models, I continued to analyze them to finally chose the best model. As we know, the mean absolute error (MAE) and the root mean square error (RMSE) are two fo the most common metrics to measure the accuracy of a regression model.
Since the MAE delivers an average value of the errors of the prediction, it is best to select a model where the MAE value is small. In other words, with a low MAE, the magnitude of the error in the forecast is tiny, making the prediction closer to the real value. Following these criteria, I chose the random forest model, which has an MAE equal to 48.85.
Additionally, I proceed to analyze the RMSE. This metric also measures the average magnitude of the errors by taking the square root of the average of squared differences between the predicted value and actual observation, meaning it will give high weight to large errors. In other words, with a smaller RMSE, we can capture fewer errors in the model. With that said, the random forest was the correct option. Additionally, the difference between the MAE and RMSE of the random forest is small, meaning that the variance in the individual error is lower.
Now that I know that the random forest model is the best one, it is required to save the model in a file to use it in the Shiny application.


```{r}
#Saving the model
saveRDS(rf, file = "./rf.rda")
```

EDA in R Shiny
In this step of the project, I built an R Shiny application where I can perform a univariate analysis of the numeric and categorical independent variables. Also, I developed an interactive dashboard to answer the business questions presented at the beginning of the article.
In a new R document, I proceed to create the R Shiny applications. First, I loaded the necessary libraries used throughout the Shiny application and the datasets.
```{r}
#Load libraries
library(shiny)
library(shinydashboard)
library(ggplot2)
library(dplyr)
#Importing datasets
bike <- read.csv('bike_sharing.csv')
bike$yr <- as.factor(bike$yr)
bike$mnth <- factor(bike$mnth, levels = c('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'))
bike$weekday <- factor(bike$weekday, levels = c('Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'))
bike$season <- factor(bike$season, levels = c('Spring', 'Summer', 'Fall', 'Winter'))
head(bike)

```













